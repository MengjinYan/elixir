Elixir is a work scheduling framework for video processing pipelines. It runs on parallel machines with multiple CPU cores and hyperthreads. The main goal of our project is to achieve a speedup, compared against the original scheduler running on [Scanner](https://github.com/scanner-research/scanner).

The code is currently located in the [Surround360](https://github.com/albusshin/Surround360) repository.

## Challenges

Many researches were conducted on scheduling policies on parallel machines. Given the model, the problem of achieving the optimal performance is NP-Hard[^1]. Hence, the first and biggest challenge for this project was to extract the features of classic video processing applications, and apply the effective and efficient heuristics on the scheduling policies.

Secondly, a scheduler should also be able to quickly decide which job to run when being asked. The policy should be simple enough, yet efficient for resources usage.

Last but not least, the implementation of the scheduler should be efficient while running on parallel machines. The overhead introduced by the scheduler should be as small as possible. This includes using the correct synchronization primitives, using the caches more efficiently, etc..

## Results with respect to Scanner

We achieved a **6.15x speedup** was achieved when running Surround360 pipeline on our scheduling policy comparing that of Scanner's.

### The Application: Surround360 by Facebook
We used the [Surround360](https://github.com/albusshin/Surround360) video processing application for the following analysis. The application provides the most complex workflow pipeline amongst all the applications written with Scanner, and the opportunities for parallelism are abundant.

<img src="i/data-flow.png" width="450" alt="Surround360 Pipeline"/>

As shown in the Pipeline figure above, the pipeline of Surround360 is consisted of 5 layers of kernels: I, P, F, R and C. The dependencies between the kernels and layers of kernels can be expressed as a DAG: the nodes representing the minimal granularity of jobs that need to be done, and the edges representing the dependencies, or data flows, between the jobs. Our job then, is to operate on the DAG and choose an efficient scheduling policy such that the C kernels (the last layer of the kernels) can be finished as early as possible.

<img src="i/dependency.png" width="450" alt="Dependencies" />

The figure above is a subsection of a flatten version of the first figure. In total, there are 14 of I kernels, 14 of P kernels, 14 of F kernels, 14 of R kernels, and 2 of C kernels. The data flows from I kernels into the whole DAG, and the output is generated by the 2 C kernels. To better understand how the data flows inside the DAG, consider one frame of 14 videos generated by the Facebook Surround360 Camera: the camera has 14 cameras on the side. Each frame generated by the 14 cameras follows into the corresponding I kernels of the pipeline, and then flows into the corresponding P kernel underneath it, to generate a projected spherical frame; and then the projected frame flows into the left and right F kernels underneath it, generating the left and right optical flows, which in turn flow into the R kernels, generating the overlapped regions in the panorama image, which are consumed by the 2 C kernels generating panorama images for both left eye and right eye.

<img src="i/camera.png" width="300" alt="Surround360 Camera" />

One important thing to notice for this workflow is, the workflow of the F kernels is serial. That is, F kernels for a single camera cannot be executed in parallel. This is because there exist dependencies between each layer of the F kernel: a next layer of the F kernel needs to use the output from the previous layer.

Given this complex workflow pipeline, we optimized our scheduling policy and achieved a reasonable speedup.

### Results

We ran our experiments on an AWS c4.8xlarge instance, with 16 cores with hyperthreading (32 vCPUs) and 244GB of Memory.

We introduce 1 baseline and 1 optimal objective here to reference the performance of our scheduler to. We ran a processing job of 20 frames from the video dataset, with each batch (layer) containing 10 frames to process.

#### Baseline: Scanner

The first baseline introduced here is running this pipeline on Scanner.

Running 20 frames used 715.42 seconds in total, and running the kernels alone (excluding the overheads and loading data from DB) used 689 seconds.

The following figure shows how much time each kernel are count for.

<img src="i/fused-4.png" width="450" alt="Scanner Baseline" />

#### Objective: Scanner with hand-tuned scheduling policy

The second baseline is running this pipeline with hand-tuned scheduling policy. We feed Scanner the jobs to run based on our pre-known knowledge about the graph, in a fashion that it can achieve the optimal performance.

Running 20 frames used 144.26 seconds in total, and running the kernels alone (excluding the overheads and loading data from DB) used 114 seconds.

The following figure shows how much time each kernel are count for.

<img src="i/fused-3.png" width="450" alt="Goal" />

#### Elixir Results

The results we achieved on running the kernels on Elixir achieved the following results.

Running 20 frames used 126 seconds in total, and running the kernels alone (excluding the I kernels running time) used 112 seconds.

Figure 5 shows how much time each kernel are count for.

<img src="i/elixir-20.png" width="450" alt="Elixir Results" />

This achieved a **689 / 112 = 6.15x speedup**

##### Speedup explained

The first reason of achieving this speedup is utilizing the fact that there are a lot of parallelism opportunities in this workflow pipeline.

Secondly, as we can see from the figures above, the F kernels are always using up most of the time for doing the computations, and it is the serialized part of the whole pipeline. According to Amdahl's law, the best case we can achieve is to use the computation time for F kernels to hide all the computation time spent in the other kernels, which is exactly what we did according to our scheduling policy.

### Design and implementation

<img src="i/system-design.png" width="450" alt="System Design" />

The system design and control flow of the Elixir scheduling framework is shown in Figure 6 and 7. The design and implementation will be discussed in more details in the final project report.

<img src="i/control-flow.png" width="450" alt="Control Flow" />

### References

[^1]: Kwok, Yu-Kwong, and Ishfaq Ahmad. "Static scheduling algorithms for allocating directed task graphs to multiprocessors." ACM Computing Surveys (CSUR) 31.4 (1999): 406-471.

